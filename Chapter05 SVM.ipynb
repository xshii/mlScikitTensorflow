{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Support Vector Machines \n",
    "linear or nonlinear classification, regression, and even\n",
    "outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-SVM-Classification---svm:LinearSVC\" data-toc-modified-id=\"Linear-SVM-Classification---svm:LinearSVC-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear SVM Classification - <code>svm:LinearSVC</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Soft-vs.-Hard-Margin-Classification---LinerSVC(C=1)\" data-toc-modified-id=\"Soft-vs.-Hard-Margin-Classification---LinerSVC(C=1)-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Soft vs. Hard Margin Classification - <code>LinerSVC(C=1)</code></a></span></li><li><span><a href=\"#scikit-SVC---svm:LinearSVC,-SVC,-SGDClassifier\" data-toc-modified-id=\"scikit-SVC---svm:LinearSVC,-SVC,-SGDClassifier-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>scikit SVC - <code>svm:LinearSVC</code>, <code>SVC</code>, <code>SGDClassifier</code></a></span></li><li><span><a href=\"#other-options\" data-toc-modified-id=\"other-options-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>other options</a></span></li></ul></li><li><span><a href=\"#Nonlinear-SVM-Classification---preprocessing:PolynomialFeatures\" data-toc-modified-id=\"Nonlinear-SVM-Classification---preprocessing:PolynomialFeatures-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Nonlinear SVM Classification - <code>preprocessing:PolynomialFeatures</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Polynomial-Kernal---kernel-trick,-svm:SVC(kernel=''poly'')\" data-toc-modified-id=\"Polynomial-Kernal---kernel-trick,-svm:SVC(kernel=''poly'')-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Polynomial Kernal - <em>kernel trick</em>, <code>svm:SVC(kernel=''poly'')</code></a></span></li><li><span><a href=\"#Add-Similarity-Features\" data-toc-modified-id=\"Add-Similarity-Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Add Similarity Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Similarity-function---RBF-Gaussian-Radial-Basis-Function\" data-toc-modified-id=\"Similarity-function---RBF-Gaussian-Radial-Basis-Function-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Similarity function - RBF Gaussian Radial Basis Function</a></span></li><li><span><a href=\"#Gaussian-RBF-Kernel---svm:SVC(kernel='rbf',gamma=5,c=0.001)\" data-toc-modified-id=\"Gaussian-RBF-Kernel---svm:SVC(kernel='rbf',gamma=5,c=0.001)-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Gaussian RBF Kernel - <code>svm:SVC(kernel='rbf',gamma=5,c=0.001)</code></a></span></li><li><span><a href=\"#other-kernels\" data-toc-modified-id=\"other-kernels-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>other kernels</a></span></li></ul></li><li><span><a href=\"#Computational-Complexity\" data-toc-modified-id=\"Computational-Complexity-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Computational Complexity</a></span><ul class=\"toc-item\"><li><span><a href=\"#LinearSVC---$O(m\\times-n)$,-tol\" data-toc-modified-id=\"LinearSVC---$O(m\\times-n)$,-tol-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>LinearSVC - $O(m\\times n)$, <code>tol</code></a></span></li><li><span><a href=\"#SVC---$O(m^2\\times-n)\\sim-O(m^3\\times-n)$\" data-toc-modified-id=\"SVC---$O(m^2\\times-n)\\sim-O(m^3\\times-n)$-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>SVC - $O(m^2\\times n)\\sim O(m^3\\times n)$</a></span></li></ul></li></ul></li><li><span><a href=\"#SVM-Regression\" data-toc-modified-id=\"SVM-Regression-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>SVM Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Linear-SVM-Regression---svm:LinearSVR(epsilon=1.5)\" data-toc-modified-id=\"Linear-SVM-Regression---svm:LinearSVR(epsilon=1.5)-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Linear SVM Regression - <code>svm:LinearSVR(epsilon=1.5)</code></a></span></li><li><span><a href=\"#Nonlinear-SVM-Regression---svm:SVR(kernel='poly',degree=2,C=100,-epsilon=0.1)\" data-toc-modified-id=\"Nonlinear-SVM-Regression---svm:SVR(kernel='poly',degree=2,C=100,-epsilon=0.1)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Nonlinear SVM Regression - <code>svm:SVR(kernel='poly',degree=2,C=100, epsilon=0.1)</code></a></span></li></ul></li><li><span><a href=\"#SVM--in-depth\" data-toc-modified-id=\"SVM--in-depth-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>SVM  in depth</a></span><ul class=\"toc-item\"><li><span><a href=\"#Decision-Functions-and-Predictions\" data-toc-modified-id=\"Decision-Functions-and-Predictions-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Decision Functions and Predictions</a></span></li><li><span><a href=\"#Training-Objective\" data-toc-modified-id=\"Training-Objective-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Training Objective</a></span><ul class=\"toc-item\"><li><span><a href=\"#hard-margin\" data-toc-modified-id=\"hard-margin-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>hard margin</a></span></li></ul></li><li><span><a href=\"#soft-margin---slack-variable\" data-toc-modified-id=\"soft-margin---slack-variable-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>soft margin - <em>slack variable</em></a></span></li><li><span><a href=\"#Quadratic-Programming\" data-toc-modified-id=\"Quadratic-Programming-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Quadratic Programming</a></span><ul class=\"toc-item\"><li><span><a href=\"#primal-problem\" data-toc-modified-id=\"primal-problem-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>primal problem</a></span></li><li><span><a href=\"#The-Dual-Problem---enable-kernel-trick\" data-toc-modified-id=\"The-Dual-Problem---enable-kernel-trick-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>The Dual Problem - enable <em>kernel trick</em></a></span></li></ul></li><li><span><a href=\"#Kernelized-SVM---Kernel-trick\" data-toc-modified-id=\"Kernelized-SVM---Kernel-trick-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Kernelized SVM - <em>Kernel trick</em></a></span></li><li><span><a href=\"#Online-SVMs\" data-toc-modified-id=\"Online-SVMs-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Online SVMs</a></span></li><li><span><a href=\"#Other-online-SVM\" data-toc-modified-id=\"Other-online-SVM-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Other online SVM</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification - `svm:LinearSVC` \n",
    "* **large margin classification** :fits the widest possible street between the classes\n",
    "* **support vectors** the instances located on the edge of the street will fully determine the decision boundary.\n",
    "* **sensitive to feature scales**. use `StandardScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft vs. Hard Margin Classification - `LinerSVC(C=1)`\n",
    "* **hard margin classification** strictly impose that all instances be off the street and on the right side. \n",
    "    * Two issues\n",
    "        1. it only works on linearly separable data\n",
    "        2. sensitive to outliers\n",
    "* **soft margin classification** keep the street as large as possible and limiting the **margin violations**\n",
    "    * **margin violations** instances that end up in the middle of the street or even on the wrong side\n",
    "    * controlled by `C` hyperparameter: a smaller `C` value leads to a wider street but more margin violations\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit SVC - `svm:LinearSVC`, `SVC`, `SGDClassifier` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`svm:LinearSVC` \n",
    "* regularizes the bias term, so center the training set first using `preprocessor:StandardScaler`.\n",
    "* set `loss='hinge'` because it is not a default value\n",
    "* for better performance, set `dual=False` unless there are more features than training stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T09:38:33.776783Z",
     "start_time": "2018-10-21T09:38:32.479557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)] # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica\n",
    "svm_clf = Pipeline((\n",
    "(\"scaler\", StandardScaler()),\n",
    "(\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")),\n",
    "))\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T09:38:41.521048Z",
     "start_time": "2018-10-21T09:38:41.508690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other options\n",
    "* use `SVC` class, \n",
    "    * `SVC(kernel=\"linear\",C=1)`, much slower for large training sets\n",
    "\n",
    "* use `SGDClassifier` class, \n",
    "    * `SGDClassifier(loss='hinge',alpha=1/(m*C))`, slower convergence than `linearSVC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification - `preprocessing:PolynomialFeatures`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply a `PolynomialFeatures` transformer followed by a `StandardScaler` and a `LinearSVC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T09:43:34.314596Z",
     "start_time": "2018-10-21T09:43:34.299722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly_features', PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), ('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "polynomial_svm_clf = Pipeline(\n",
    "    ((\"poly_features\", PolynomialFeatures(degree=3)), \n",
    "     (\"scaler\", StandardScaler()),\n",
    "     (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))))\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernal - *kernel trick*, `svm:SVC(kernel=''poly'')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* at a low polynomial degree, it cannot deal with complex datasets,\n",
    "* with a high polynomial degree, it creates a huge number of features, making the model too slow.\n",
    "* miraculous mathematical technique - **kernel trick**\n",
    "    * it makes it possible to get the same result as if you added many polynomial features, **without actually** having to add them.\n",
    "    * <span class=\"girk\">this trick is implemented by the `SVC` class.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T09:51:48.816727Z",
     "start_time": "2018-10-21T09:51:48.804397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=5, cache_size=200, class_weight=None, coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "poly_kernel_svm_clf = Pipeline((\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
    "))\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `kernel='poly', degree=3` use a $3^{rd}$-degree polynomial kernel \n",
    "* `coef0=1` controls how much the model is influenced by high-degree polynomials vs. low-degree polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Add Similarity Features\n",
    "add features computed using a *similarity function* that measures how much each instance resembles a particular *landmark*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity function - RBF Gaussian Radial Basis Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gaussian Radial Basis Function**\n",
    "    * $\\phi\\gamma(x,l)=\\exp(-\\gamma||x-l||^2)$\n",
    "    * it is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark)\n",
    "    * $x_{new_i}=\\phi(x_{old},landmark_i)$, and the old features are discarded\n",
    "    \n",
    "* how to select the *landmarks*\n",
    "    * the simplest approach is to create a landmark at the location of each and every instance in the dataset.\n",
    "        * pros: this creates many dimensions and thus increases the chances that the transformed training set will be linearly separable\n",
    "        * cons: a training set with $m$ instances and $n$ features get transformed into a training set with $m$ instances and $m$ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian RBF Kernel - `svm:SVC(kernel='rbf',gamma=5,c=0.001)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it applies the kernel trick again. It makes it possible to obtain a similar result as if you had added many similarity features, without actually having to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T09:56:14.116261Z",
     "start_time": "2018-10-21T09:56:14.107443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svm_clf', SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline((\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
    "))\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `kernel='rbf', gamma=5`, \n",
    "    * overfitting: large `gamma` makes the bell-shape curve narrower and as a result each instance's range of influence is smaller. And the decision boundary ends up being more irregular, wiggling around individual instances\n",
    "    * underfitting: small `gamma` make the bell-shaped curve wider and decision boundary smoother.\n",
    "* `C=0.001`,\n",
    "    * overfitting: large `C`\n",
    "    * underfitting: small `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **string** kernels when classifying <span class=\"mark\">text documents</span> or <span class=\"mark\">DNA sequences</span> (use the *string subsequence kernel* or kernels based on the *Levenshtein distance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC - $O(m\\times n)$, `tol`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearSVC` class is based on the `liblinear` library, which implements an optimized algorithm for linear SVMs. (\"A Dual Coordinate Descent Method for Large-scale Linear SVM\")\n",
    "\n",
    "It does **not** support the kernel trick, but it scales almost linearly with the number of training instances and the number of features, i.e. \n",
    "\n",
    "* The time complexity is roughly $O(m\\times n)$\n",
    "* the algorithm takes longer with higher tolerance hyperparameter $\\eta$ (`tol` in Scikit-Learn. In most cases, the default tolerance is fine.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC - $O(m^2\\times n)\\sim O(m^3\\times n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SVC` class is based on the `libsvm` library, which implements an algorithm that supports the *kernel trick*. (*“Sequential Minimal Optimization (SMO),” J. Platt (1998).*)\n",
    "\n",
    "* the training time complexity is usually between $O(m^2\\times n)$ and $O(m^3\\times n)$\n",
    "    * cons: it suits for complex but small training sets.\n",
    "    * pros: it scales well with the number of features, especially with *sparse features* (few nonzero features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression \n",
    "reverse the objective, fit as many instances as possible **on the street** while limiting margin violations (instances *off* the street)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\epsilon$ `epsilon=1.5` hyperparameter for the street width\n",
    "    * **$\\epsilon-insensitive$** : adding more training instances within the margin does not affect the model's predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM Regression - `svm:LinearSVR(epsilon=1.5)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T12:04:18.873930Z",
     "start_time": "2018-10-21T12:04:18.855494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "     intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "     random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Regression - `svm:SVR(kernel='poly',degree=2,C=100, epsilon=0.1)` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `C`, higher `C` less regularization, less smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-23T16:00:58.978952Z",
     "start_time": "2017-11-23T16:00:58.293466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='auto',\n",
       "  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM  in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* bias term $b$\n",
    "* the feature weights vector $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Functions and Predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **decision function**\n",
    "    * $w^Tx+b=w_1x_1+\\cdots+w_nx_n+b$\n",
    "* **Linear SVM classifier prediction**\n",
    "    * $\\hat{y} = 0$ if $w^Tx+b<0$\n",
    "    * $\\hat{y} = 1$ if $w^Tx+b\\geq0$ \n",
    "* **margin**\n",
    "    * $-1 \\leq w^Tx+b\\leq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a linear SVM classifier means finding the value of $w$ and $b$ that\n",
    "make this margin **as wide as possible** while *avoiding margin violations* (hard margin)\n",
    "or *limiting them* (soft margin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* slope of the decision function = $||w||$\n",
    "    * larger slope, narrower width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hard margin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $t^{(i)}=-1$ for negative instances ($y^{(i)}=0$)\n",
    "* $t^{(i)}=1$ for positive instances ($y^{(i)}=1$)\n",
    "* constraint $t^{(i)}(w^Tx^{(i)}+b)\\geq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimize$(w,b)$ $\\frac{1}{2}w^Tw$\n",
    "\n",
    "subject to $t^{(i)}(w^Tx^{(i)}+b)\\geq 1$ for $i=1,2,\\cdots,m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft margin - *slack variable* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "introduce a **slack variable** $\\zeta^{(i)}\\geq 0$ for each instance.\n",
    "\n",
    "* $\\zeta^{(i)}$ measures how much the $i^{th}$ instance is allowed to violate the margin.\n",
    "* `C` hyperparameter comes in: it allow the definition of the trade-off between two objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimize$(w,b)$ $\\frac{1}{2}w^Tw + C\\sum_{i=1}^m \\zeta^{(i)}$\n",
    "\n",
    "subject to $t^{(i)}(w^Tx^{(i)}+b)\\geq 1-\\zeta^{(i)}$ and $\\zeta^{(i)}\\geq 0$ for $i=1,2,\\cdots,m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Programming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Quadratic Programming (QP) problems**: The *hard margin* and *soft margin* problems are both *convex quadratic optimization* problems with *linear constraints*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### primal problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minimize$(p)$ $\\frac{1}{2}p^THp + f^Tp$\n",
    "\n",
    "subject to $Ap\\leq b$\n",
    "\n",
    "where\n",
    "* $p \\in R ^{n_p}$\n",
    "    * $n_p=n+1$ $n$ is the number of features\n",
    "* $H \\in R^{n_p\\times n_p}$, \n",
    "    * $H=I$ except $H_{00}=0$ (ignore the bias term)\n",
    "* $f \\in R^{n_p}$\n",
    "    * $f=0$\n",
    "* $A \\in R^{n_c\\times n_p}$ , $n_c=m$ is the number of constraints, $m$ is the number of training instances\n",
    "    * $a^{(i)}=-t^{(i)}x^{(i)}$ where $x^{(i)}$ includes extra bias feature $x_0=1$\n",
    "* $b \\in R^{n_c}$\n",
    "    * $b=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dual Problem - enable *kernel trick*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the dual problem typically gives **a lower bound** to the solution of the primal problem, but under some conditions (SVM meets) it can even have **the same solutions** as the primal problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Dual form of the linear SVM objective\n",
    "\n",
    "minimize($\\alpha$) $\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m \\alpha^{(i)} \\alpha^{(j)} t^{(i)} t^{(j)} x_{(i)}^T x^{(j)}-\\sum_{i=1}^m \\alpha^{(i)}$\n",
    "\n",
    "subject to $\\alpha^{(i)}\\geq 0$  for $i=1,2,\\cdots,m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the dual solution to the primal solution 4.4.2\n",
    "\n",
    "$\\hat{w}=\\sum_{i=1}^m\\hat{\\alpha^{(i)}}t^{(i)}x^{(i)}$\n",
    "\n",
    "$\\hat{b}=\\frac{1}{n_s}\\sum_{i=1 \\& \\hat{\\alpha^{(i)}}>0}^m(1-t^{(i)}(\\hat{w}^Tx^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem is **faster to solve** than the primal when the **number of training**\n",
    "instances is **smaller** than **the number of features** $m<n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized SVM - *Kernel trick* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Second-degree polynomial mapping**\n",
    "    * $\\phi(x)=\\phi((x_1,x_2)^T)=(x_1^2,\\sqrt{2}x_1x_2,x_2^2)^T$\n",
    "* **Kernel trick for a $2^{nd}$-degree polynomial mapping**\n",
    "    * $\\phi(a)^T\\phi(b) = (a^T\\cdot b)^2$\n",
    "    * if you apply the transformation $\\phi$ to all training instances, then the dual problem will contain the dot product $\\phi(x^{(i)})^T\\phi(x^{(j)})$. And this is exactly $({x^{(i)}}^T\\cdot x^{(j)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common kernels\n",
    "    * Linear $K(a,b)=a^Tb$\n",
    "    * Polynomial $K(a,b)=(\\gamma a^Tb+r)^d$\n",
    "    * Gaussian RBF $K(a,b)=\\exp(-\\gamma||a-b||^2)$\n",
    "    * Sigmoid $K(a,b)=\\tanh(\\gamma a^Tb+r)$ \n",
    "    \n",
    "* **Mercer's Theorem**\n",
    "    * **Mercer's conditions**, $K$ must be continuous, symmetric, $K(a,b)=K(b,a)$\n",
    "    * then there exists $\\phi$ such that $K(a,b)=\\phi(a)^T\\phi(b)$\n",
    "    * you don't have to know what $\\phi$ is. Like *Gaussian RBF kernel*\n",
    "    * *Sigmoid kernel* don't respect all of Mercer's condition, yet they generally work well in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make predictions with a **kernelized SVM**\n",
    "\n",
    "$h_{\\hat{w},\\hat{b}}(\\theta(x^{n})) = {plug\\,into\\,4.4.2} = \\sum_{i=1 \\& \\hat{\\alpha}^{(i)}>0}^m \\hat{\\alpha}^{(i)}t^{(j)}K(x^{(i)},x^{(n)})+\\hat{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha^{(i)}\\neq 0 $ only for support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compute the bias term $b$ using the *kernel trick*\n",
    "\n",
    "$\\hat{b}= {plug\\,into\\,4.4.2} = \\frac{1}{n_s}\\sum_{i=1 \\& \\hat{\\alpha^{(i)}}>0}^m(1-\\hat{\\alpha}^{(j)}t^{(i)}K(x^{(i)},x^{(j)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online SVMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately SGD converges much more slowly than the methods based on QP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Linear SVM classifier cost function**\n",
    "\n",
    "$J(w,b)=\\frac{1}{2}w^Tw+C\\sum_{i=1}^m\\max(0,1-t^{(i)}(w^Tx^{(i)}+b))$\n",
    "\n",
    "* the first sum to enlarge the margin\n",
    "* the second sum computes the total of all margin violations it is also called **Hinge Loss**\n",
    "    * $\\max(0,1-t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other online SVM\n",
    "* [\"Incremental and Decremental SVM Learning\"](http://www.isn.ucsd.edu/papers/nips00_inc.pdf)\n",
    "* [\"Fast Kernel Classifiers with Online and Active Learning.\"](http://www.isn.ucsd.edu/papers/nips00_inc.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
