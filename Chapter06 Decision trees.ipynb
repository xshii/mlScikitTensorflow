{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification, regression, multioutput tasks\n",
    "\n",
    "* require very little data preparation. In particular, they don't require feature scaling or centering at all\n",
    "* *CART* algorithm produces only *binary trees*, *ID3* produces trees with more branches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Training-and-visualizing-a-decision-tree\" data-toc-modified-id=\"Training-and-visualizing-a-decision-tree-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Training and visualizing a decision tree</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-example-of-a-decision-tree---tree:DecisionTreeClassifier\" data-toc-modified-id=\"An-example-of-a-decision-tree---tree:DecisionTreeClassifier-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>An example of a decision tree - <code>tree:DecisionTreeClassifier</code></a></span></li><li><span><a href=\"#Visualization-of-a-tree---tree:export_graphviz\" data-toc-modified-id=\"Visualization-of-a-tree---tree:export_graphviz-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Visualization of a tree - <code>tree:export_graphviz</code></a></span></li><li><span><a href=\"#Making-predictions---.predict_proba(),.predict()\" data-toc-modified-id=\"Making-predictions---.predict_proba(),.predict()-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Making predictions - <code>.predict_proba()</code>,<code>.predict()</code></a></span></li></ul></li><li><span><a href=\"#The-CART-training-algorithm\" data-toc-modified-id=\"The-CART-training-algorithm-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The CART training algorithm</a></span><ul class=\"toc-item\"><li><span><a href=\"#CART-cost-function-for-classification\" data-toc-modified-id=\"CART-cost-function-for-classification-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>CART cost function for classification</a></span></li><li><span><a href=\"#Computational-complexity\" data-toc-modified-id=\"Computational-complexity-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Computational complexity</a></span></li><li><span><a href=\"#Gini-impurity-or-entropy\" data-toc-modified-id=\"Gini-impurity-or-entropy-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Gini impurity or entropy</a></span></li></ul></li><li><span><a href=\"#Regularization-hyperparameters\" data-toc-modified-id=\"Regularization-hyperparameters-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Regularization hyperparameters</a></span><ul class=\"toc-item\"><li><span><a href=\"#pruning-algorithm\" data-toc-modified-id=\"pruning-algorithm-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>pruning algorithm</a></span></li></ul></li><li><span><a href=\"#Regression---tree:DecisionTreeRegressor\" data-toc-modified-id=\"Regression---tree:DecisionTreeRegressor-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Regression - <code>tree:DecisionTreeRegressor</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#CART-cost-function-for-regression\" data-toc-modified-id=\"CART-cost-function-for-regression-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>CART cost function for regression</a></span></li></ul></li><li><span><a href=\"#Instability\" data-toc-modified-id=\"Instability-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Instability</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and visualizing a decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of a decision tree - `tree:DecisionTreeClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T14:12:40.118903Z",
     "start_time": "2018-10-21T14:12:40.101169Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:,2:]\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of a tree - `tree:export_graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T14:16:45.888743Z",
     "start_time": "2018-10-21T14:16:45.873735Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "            tree_clf,\n",
    "            out_file=\"./iris_tree.dot\",\n",
    "            feature_names=iris.feature_names[2:],\n",
    "            class_names=iris.target_names,\n",
    "            rounded=True,\n",
    "            filled=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then convert this *.dot* file to PDF or PNG using the `dot` command-line tool from the `graphviz` package\n",
    "```[bash]\n",
    "$ dot -Tpng iris_tree.dot-o iris_tree.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions - `.predict_proba()`,`.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the explanation of attributes\n",
    "* `samples`: counts how many training instances it applies to\n",
    "* `value`: how many training instances of each class this node applied to\n",
    "* `gini`: the impurity\n",
    "$$G_i=1-\\sum_{k=1}^{n}p_{i,k}^2$$\n",
    "    * (`gini=0` ) the node is \"pure\"\n",
    "    * $p_{i,k}$ is the ratio of class $k$ instances among the training instances in the $i^{th}$ node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T14:42:42.970214Z",
     "start_time": "2018-10-21T14:42:42.948328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.90740741, 0.09259259]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict_proba([[5, 1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T14:43:06.191701Z",
     "start_time": "2018-10-21T14:43:06.181140Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.predict([[5,1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the *Classification And Regression Tree* (CART), a *greedy* algorithm\n",
    "\n",
    "<span class=\"burk\">**Finding the optimal tree is known to be an *NP-Complete* problem, which requires $O(\\exp(m))$ time.**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART cost function for classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(k,t_k)=\\frac{m_{left}}{m}G_{left}+\\frac{m_{right}}{m}G_{right}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $G_*$ measures the impurity of the * subset\n",
    "* $m_*$ is the number of instances in the * subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stops recursing once\n",
    "* it reaches the maximum depth `max_depth`\n",
    "* it cannot find a split that will reduce impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction ~ $O(\\log_2(m))$\n",
    "\n",
    "training ~ $O(n\\times m\\log(m))$ or less if `max_features` is set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini impurity or entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy\n",
    "$$H_i=-\\sum_{k=1,p_{i,k}\\neq 0}^{n}p_{i,k}\\log(p_{i,k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gini impurity is slightly faster to compute, so it is a good default\n",
    "* Gini impurity tends to isolate the most frequent class in its won branch of the tree, while entropy tends to produce slightly more balanced trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `max_depth`: default is `None` which leads to *nonparametric model*. Set it to restrict the maximum depth of the Decision tree.\n",
    "* `min_sample_split`: the minimum number of samples a node must have before it can be split\n",
    "* `min_samples_leaf`: the minimum number of samples a leaf node must have.\n",
    "* `min_weight_fraction_leaf`: expressed as a fraction of the total number of weighted instances.\n",
    "* `max_leaf_nodes`: maximum number of leaf nodes\n",
    "* `max_features`: maximum number of features that are evaluated for splitting at each node\n",
    "\n",
    "increasing `min_*` or reducing `max_*` results in regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pruning algorithm\n",
    "first training the decision tree without restrictions, then deleting unnecessary nodes.\n",
    "\n",
    "A node whose children are all leaf nodes is considered unnecessary if the purity improvement it provides is not *statistically significant*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression - `tree:DecisionTreeRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-21T15:09:58.741162Z",
     "start_time": "2018-10-21T15:09:58.729005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=2, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimize the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART cost function for regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J(k,t_k)=\\frac{m_{left}}{m}MSE_{left}+\\frac{m_{right}}{m}MSE_{right}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "* $MSE_* = \\sum_{i\\in node}(\\hat{y}_{node}-y^{(i)})^2$\n",
    "* $\\hat{y}_{node}=\\frac{1}{m_{node}}\\sum_{i\\in node}y^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision trees love orthogonal decision boundaries, which makes them sensitive to training set rotation.\n",
    "    * use *PCA* (chapter 8) to get a better orientation of the training data\n",
    "* very sensitive to small variations in the training data\n",
    "* stochastic training algorithm, \n",
    "    * set `random_state` to get the same model\n",
    "    * use *Random Forests* model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
